{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mamba_ssm import Mamba2\n",
    "import torch\n",
    "\n",
    "\n",
    "x = torch.randn(1, 4096, 512).cuda()\n",
    "\n",
    "model = Mamba2(\n",
    "    # This module uses roughly 3 * expand * d_model^2 parameters\n",
    "    d_model=512,  # Model dimension d_model\n",
    "    d_state=64,  # SSM state expansion factor, typically 64 or 128\n",
    "    d_conv=4,  # Local convolution width\n",
    "    expand=2,  # Block expansion factor\n",
    "    layer_idx=0,\n",
    ").to(\"cuda\")\n",
    "y = model(x)\n",
    "assert y.shape == x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Reference 2: Generate the last @genlen tokens of each sequence in a for loop\\nout_loop = []\\nfor input_ids in xs:\\n    out = model.generate(\\n        input_ids=input_ids[:, :-genlen], max_length=input_ids.shape[1], output_scores=True,\\n        return_dict_in_generate=True, cg=True, teacher_outputs=input_ids,\\n    ).scores\\n    out_loop.append(torch.stack(out, dim=1))\\nout_loop = torch.cat(out_loop, dim=0)\\nprint(f\"Max diff between ref1 and ref2: {(out_loop - out_ref).abs().max()}\")'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "seqlens = [170, 65, 100]\n",
    "genlen = 20\n",
    "total_seqlen = sum(seqlens)\n",
    "device = \"cuda\"\n",
    "dtype = torch.float16\n",
    "\n",
    "config = MambaConfig(\n",
    "    d_model=1024,\n",
    "    n_layer=4,\n",
    "    vocab_size=32000,\n",
    "    ssm_cfg=dict(layer=\"Mamba2\"),\n",
    "    rms_norm=True,\n",
    "    residual_in_fp32=True,\n",
    "    fused_add_norm=True,\n",
    "    pad_vocab_size_multiple=16,\n",
    ")\n",
    "torch.manual_seed(2357)\n",
    "model = MambaLMHeadModel(config, device=device, dtype=dtype)\n",
    "xs = [torch.randint(0, 1000, (1, seqlen), device=device, dtype=torch.long) for seqlen in seqlens]\n",
    "\n",
    "# Reference 1: Forward pass with seq_idx\n",
    "x = torch.cat(xs, dim=1)\n",
    "seq_idx = torch.cat([torch.full((ids.shape[1],), i, dtype=torch.int32, device=device)\n",
    "                        for i, ids in enumerate(xs)], dim=0).unsqueeze(0)\n",
    "cu_seqlens = F.pad(torch.tensor(seqlens, device=device, dtype=torch.int32).cumsum(dim=0), (1, 0))\n",
    "out_ref = model(x, seq_idx=seq_idx).logits\n",
    "# Only take the last @genlen logits of each sequence\n",
    "out_ref = torch.cat([out_ref[:, cu_seqlens[i + 1] - genlen - 1:cu_seqlens[i + 1] - 1]\n",
    "                        for i in range(len(seqlens))], dim=0)\n",
    "\"\"\"\n",
    "# Reference 2: Generate the last @genlen tokens of each sequence in a for loop\n",
    "out_loop = []\n",
    "for input_ids in xs:\n",
    "    out = model.generate(\n",
    "        input_ids=input_ids[:, :-genlen], max_length=input_ids.shape[1], output_scores=True,\n",
    "        return_dict_in_generate=True, cg=True, teacher_outputs=input_ids,\n",
    "    ).scores\n",
    "    out_loop.append(torch.stack(out, dim=1))\n",
    "out_loop = torch.cat(out_loop, dim=0)\n",
    "print(f\"Max diff between ref1 and ref2: {(out_loop - out_ref).abs().max()}\")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2952,  1.2607, -0.2241,  ...,  0.5239, -1.0312,  0.4465],\n",
       "         [-1.6006,  0.3271, -0.1626,  ...,  0.7397,  0.6523, -1.1016],\n",
       "         [-0.3184, -0.0527,  0.2690,  ..., -0.1461, -0.0345, -0.3540],\n",
       "         ...,\n",
       "         [-0.9595, -0.3396,  0.9458,  ...,  0.0075, -0.5146, -0.8008],\n",
       "         [ 0.6274,  0.6328, -0.2942,  ...,  0.2147, -1.2061,  0.4158],\n",
       "         [ 0.2052, -0.8013, -0.1104,  ...,  0.2040, -0.0773, -0.1528]],\n",
       "\n",
       "        [[ 0.5171,  0.8818,  0.0328,  ...,  0.3943, -0.6523, -0.5820],\n",
       "         [ 0.2727, -0.5337, -0.0599,  ..., -0.1683,  1.1680,  0.4231],\n",
       "         [ 0.9858,  1.4727,  0.0798,  ...,  0.3040, -0.3718,  0.2317],\n",
       "         ...,\n",
       "         [ 1.2686, -1.0332, -1.0049,  ..., -1.0918,  0.3596, -0.0473],\n",
       "         [-0.6675, -0.6196,  0.0687,  ..., -1.1299, -0.7573, -0.2954],\n",
       "         [-0.3403,  0.0311,  2.2285,  ...,  0.3616,  0.4629, -0.3442]],\n",
       "\n",
       "        [[-0.9526,  0.0261,  0.4924,  ...,  0.8110, -0.7744, -0.0202],\n",
       "         [-0.5474,  0.5635,  0.6396,  ..., -0.0095,  0.1134,  0.5522],\n",
       "         [ 0.5518, -0.2141,  1.0068,  ...,  0.0047,  0.3220,  0.2500],\n",
       "         ...,\n",
       "         [ 0.6411, -0.1533,  0.5161,  ...,  0.0544, -0.3264, -0.3069],\n",
       "         [-0.3328,  0.0524,  0.2656,  ...,  0.4651, -0.6343,  1.2383],\n",
       "         [ 0.5850, -0.4236, -0.1899,  ..., -0.2416,  0.6353,  0.2708]]],\n",
       "       device='cuda:0', dtype=torch.float16, grad_fn=<CatBackward0>)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 335]), torch.Size([1, 335]))"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, seq_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 335]), torch.Size([1, 335]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape, seq_idx.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 275, 32000])\n"
     ]
    }
   ],
   "source": [
    "input_ids = torch.cat([ids[:, :-genlen] for ids in xs], dim=1)\n",
    "prompt_seqlens = [seqlen - genlen for seqlen in seqlens]\n",
    "cu_seqlens = F.pad(torch.tensor(prompt_seqlens, device=device, dtype=torch.int32).cumsum(dim=0), (1, 0))\n",
    "seq_idx = torch.cat([torch.full((seqlen,), i, dtype=torch.int32, device=device)\n",
    "                      for i, seqlen in enumerate(prompt_seqlens)], dim=0).unsqueeze(0)\n",
    "inference_params = InferenceParams(max_seqlen=2048, max_batch_size=len(seqlens))\n",
    "\n",
    "scores, sequences = [], []\n",
    "# Both seq_idx and cu_seqlens must be passed in for varlen generation\n",
    "logits = model(input_ids, inference_params=inference_params, seq_idx=seq_idx, cu_seqlens=cu_seqlens).logits\n",
    "\n",
    "print(logits.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 32000])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rearrange(logits[0, cu_seqlens[1:] - 1], \"b d -> b 1 d\").shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = rearrange(logits[0, cu_seqlens[1:] - 1], \"b d -> b 1 d\")\n",
    "scores.append(logits)\n",
    "# In practice we should sample. In this case we take from the teacher_output for testing\n",
    "sampled_tokens = rearrange(\n",
    "    torch.stack([ids[0, -genlen] for ids in xs], dim=0), \"b -> b 1\"\n",
    ")\n",
    "# sequences.append(sampled_tokens)\n",
    "# for i in range(1, genlen):\n",
    "#     inference_params.seqlen_offset += 1\n",
    "#     logits = model(\n",
    "#         sampled_tokens, inference_params=inference_params, num_last_tokens=1\n",
    "#     ).logits\n",
    "#     scores.append(logits)\n",
    "#     # In practice we should sample. In this case we take from the teacher_output for testing\n",
    "\n",
    "#     print(logits.shape)\n",
    "#     sampled_tokens = rearrange(\n",
    "#         torch.stack([ids[0, -genlen + i] for ids in xs], dim=0), \"b -> b 1\"\n",
    "#     )\n",
    "#     sequences.append(sampled_tokens)\n",
    "# out_varlen = torch.cat(scores, dim=1)\n",
    "\n",
    "# out_varlen.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n",
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fbb4578b8e0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mt.ds import build_dataset\n",
    "\n",
    "ds = build_dataset(\"iwslt17\", \"de\", \"en\", is_encoder_decoder=False)\n",
    "tokenizer = ds.get_tokenizer()\n",
    "tokenizer.padding_side = \"right\"\n",
    "tdl, vdl,_ = ds.get_dataloaders(train_batch_size=2, val_batch_size=2, tokenizer=tokenizer)\n",
    "tdl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pack_2d(tokens, cu_seqlens):\n",
    "    \"\"\"\n",
    "    pack function: convert tokens to packed_tokens (batch_size=1)\n",
    "\n",
    "    Args:\n",
    "    tokens (torch.Tensor): Input tensor of shape (batch_size, max_seq_len)\n",
    "    cu_seqlens (torch.Tensor): Cumulative sequence lengths tensor\n",
    "\n",
    "    Returns:\n",
    "    torch.Tensor: Packed tokens of shape (total_tokens,)\n",
    "    \"\"\"\n",
    "    batch_size, max_seq_len = tokens.shape\n",
    "    seq_len_list = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "\n",
    "    # Create a mask for valid tokens\n",
    "    indices_2d = (\n",
    "        torch.arange(max_seq_len, device=tokens.device)\n",
    "        .unsqueeze(0)\n",
    "        .expand(batch_size, -1)\n",
    "    )\n",
    "    mask_2d = indices_2d < seq_len_list.unsqueeze(1)\n",
    "\n",
    "    print(mask_2d)\n",
    "    # Apply the mask and flatten the result\n",
    "    packed_tokens = tokens[mask_2d]\n",
    "\n",
    "    return packed_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(packed_hidden_states, cu_seqlens):\n",
    "    batch_size = cu_seqlens.shape[0] - 1\n",
    "    seq_len = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n",
    "\n",
    "    packed_hidden_states = packed_hidden_states.squeeze(0)\n",
    "\n",
    "    ori_indices = (\n",
    "        torch.arange(seq_len, device=cu_seqlens.device)\n",
    "        .unsqueeze(0)\n",
    "        .expand((batch_size, seq_len))\n",
    "    )\n",
    "\n",
    "    ori_indices = (ori_indices + cu_seqlens[:-1].unsqueeze(1)) % (\n",
    "        len(packed_hidden_states)\n",
    "    )\n",
    "\n",
    "    return packed_hidden_states[ori_indices]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([  0,  88, 145], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 145])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1]], device='cuda:0', dtype=torch.int32)\n",
      "torch.Size([1, 145, 32000])\n",
      "torch.Size([2, 88, 32000])\n",
      "tensor(10.6094, device='cuda:0', dtype=torch.float16,\n",
      "       grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i,b in enumerate(tdl):\n",
    "    batch = {k: v.to(device) for k,v in b.items()}\n",
    "\n",
    "    ids, labels = (\n",
    "        batch[\"input_ids\"][:, :-1].contiguous(),\n",
    "        batch[\"input_ids\"][:, 1:].contiguous(),\n",
    "    )\n",
    "    attention_mask = batch[\"attention_mask\"][:, :-1]\n",
    "    batch_size = attention_mask.shape[0]\n",
    "\n",
    "    seqlens = attention_mask.sum(dim=1, dtype=torch.int32)\n",
    "    seq_idx = torch.cat(\n",
    "        [\n",
    "            torch.full((seqlen,), i, dtype=torch.int32, device=ids.device)\n",
    "            for i, seqlen in enumerate(seqlens)\n",
    "        ],\n",
    "        dim=0,\n",
    "    ).unsqueeze(0)\n",
    "    cu_seqlens = torch.zeros(\n",
    "        batch_size + 1, dtype=torch.int32, device=attention_mask.device\n",
    "    )\n",
    "    cu_seqlens[1:] = seqlens.cumsum(0)\n",
    "\n",
    "    packed_ids = ids[attention_mask.bool()].unsqueeze(0)\n",
    "    print(cu_seqlens)\n",
    "    print(packed_ids.shape)\n",
    "    print(seq_idx)\n",
    "    # break\n",
    "    lm_logits = model.forward(\n",
    "        input_ids=packed_ids,\n",
    "        seq_idx=seq_idx,\n",
    "    ).logits\n",
    "\n",
    "\n",
    "    print(lm_logits.shape)\n",
    "    unpacked = unpack(lm_logits, cu_seqlens)\n",
    "    print(unpacked.shape)\n",
    "\n",
    "    sep_mask = (ids == tokenizer.sep_token_id).cumsum(dim=1) > 0\n",
    "    labels[~sep_mask] = tokenizer.pad_token_id\n",
    "\n",
    "    loss = F.cross_entropy(\n",
    "        unpacked.view(-1, lm_logits.size(-1)),\n",
    "        labels.view(-1),\n",
    "        ignore_index=tokenizer.pad_token_id,\n",
    "    )\n",
    "    print(loss)\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([84])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.view(-1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True]], device='cuda:0')"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size, max_seq_len = ids.shape\n",
    "seq_len_list = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "\n",
    "# Create a mask for valid tokens\n",
    "indices_2d = (\n",
    "    torch.arange(max_seq_len, device=ids.device)\n",
    "    .unsqueeze(0)\n",
    "    .expand(batch_size, -1)\n",
    ")\n",
    "mask_2d = indices_2d < seq_len_list.unsqueeze(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 42, 42])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mask_2d.sum(dim=1, dtype=torch.int32), attention_mask.sum(dim=1, dtype=torch.int32)\n",
    "\n",
    "\n",
    "attention_mask = attention_mask.contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([57])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids[attention_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  565,   360,   440,   457,   741,  1130, 21312,    15,   304,  1894,\n",
       "          558,  2373,   411,  6463, 22246,    18,    27,    22,  9801,   360,\n",
       "           17,     2,   820,   354,   265,  2636, 26607,  9128,    15,  9128,\n",
       "          336,  5039,  3693,   280, 22246, 29471,    27,    22,  2373,   411,\n",
       "         6463,    17,  2981,   348,    16, 15060,  1540,  4914, 14023,    17,\n",
       "            2,  1979,  7783,   979, 30988,    17,     1,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "            0,     0,     0,     0], device='cuda:0')"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ids.view(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for tensor of dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[71], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbool\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mIndexError\u001b[0m: too many indices for tensor of dimension 1"
     ]
    }
   ],
   "source": [
    "ids.view(-1)[attention_mask.bool()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 42, 57], device='cuda:0', dtype=torch.int32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 47, 32000]) tensor([ 0, 47, 73], device='cuda:0', dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "for i,b in enumerate(vdl):\n",
    "    batch = {k: v.to(device) for k,v in b.items()}\n",
    "    input_ids, labels = batch[\"input_ids\"], batch[\"labels\"]\n",
    "\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    max_length = 20\n",
    "    attention_mask = batch[\"attention_mask\"]\n",
    "\n",
    "    done = torch.tensor([False] * batch_size).to(input_ids.device)\n",
    "\n",
    "    inference_params = InferenceParams(\n",
    "            max_seqlen=max_length + seq_len,\n",
    "            max_batch_size=batch_size,)\n",
    "    seqlens = attention_mask.sum(dim=1, dtype=torch.int32)\n",
    "    seq_idx = torch.cat(\n",
    "        [\n",
    "            torch.full((seqlen,), i, dtype=torch.int32, device=input_ids.device)\n",
    "            for i, seqlen in enumerate(seqlens)\n",
    "        ],\n",
    "        dim=0,\n",
    "    ).unsqueeze(0)\n",
    "    cu_seqlens = torch.zeros(\n",
    "        batch_size + 1, dtype=torch.int32, device=attention_mask.device\n",
    "    )\n",
    "    cu_seqlens[1:] = seqlens.cumsum(0)\n",
    "\n",
    "    packed_ids = input_ids[attention_mask.bool()].unsqueeze(0)\n",
    "    lm_logits = model.forward(\n",
    "        input_ids=packed_ids,\n",
    "        seq_idx=seq_idx,\n",
    "        cu_seqlens=cu_seqlens,\n",
    "        inference_params=inference_params,\n",
    "    ).logits\n",
    "    unpacked = unpack(lm_logits, cu_seqlens)\n",
    "\n",
    "\n",
    "    print(unpacked.shape, cu_seqlens)\n",
    "    # next_tokens = torch.argmax(unpacked[:, cu_seqlens[1:]-1, :], dim=-1, keepdim=True)\n",
    "\n",
    "    break\n",
    "\n",
    "    for i in range(1, max_length):\n",
    "        out = model.forward(\n",
    "            input_ids=next_tokens,\n",
    "            inference_params=inference_params,\n",
    "        )\n",
    "        next_tokens = torch.argmax(out.logits[:, -1, :], dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat((input_ids, next_tokens), dim=-1)\n",
    "        is_eos = next_tokens == tokenizer.eos_token_id\n",
    "        done = done | is_eos.squeeze(-1)\n",
    "        if done.all():\n",
    "            break\n",
    "\n",
    "    print(input_ids.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 32000])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm_logits[:,cu_seqlens[1:]-1].view(batch_size,1, -1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_ref = torch.cat([out_ref[:, cu_seqlens[i + 1] - genlen - 1:cu_seqlens[i + 1] - 1]\n",
    "                      for i in range(len(seqlens))], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 20, 1024])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_ref.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unpack(packed_hidden_states, cu_seqlens):\n",
    "    batch_size = cu_seqlens.shape[0] - 1\n",
    "    seq_len = (cu_seqlens[1:] - cu_seqlens[:-1]).max()\n",
    "\n",
    "    packed_hidden_states = packed_hidden_states.squeeze(0)\n",
    "\n",
    "    ori_indices = (\n",
    "        torch.arange(seq_len, device=cu_seqlens.device)\n",
    "        .unsqueeze(0)\n",
    "        .expand((batch_size, seq_len))\n",
    "    )\n",
    "\n",
    "    ori_indices = (ori_indices + cu_seqlens[:-1].unsqueeze(1)) % (\n",
    "        len(packed_hidden_states)\n",
    "    )\n",
    "\n",
    "    return packed_hidden_states[ori_indices]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "pack function: convert hidden_states to packed_hidden_states (batch_size=1)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def pack(hidden_states, cu_seqlens):\n",
    "    batch_size, seq_len, hidden_dim = hidden_states.shape\n",
    "    seq_len_list = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "    seq_len_list_3d = seq_len_list.unsqueeze(1).unsqueeze(2)\n",
    "    indices_3d = (\n",
    "        torch.arange(seq_len, device=hidden_states.device)\n",
    "        .unsqueeze(0)\n",
    "        .unsqueeze(2)\n",
    "        .repeat(batch_size, 1, hidden_dim)\n",
    "    )\n",
    "    mask_3d = indices_3d < seq_len_list_3d\n",
    "    packed_hidden_states = hidden_states[mask_3d].view(-1, hidden_dim)\n",
    "    return packed_hidden_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [5, 8, 13]\n",
    "max_seq_len = max(lens)\n",
    "seqs = [torch.randn(lens[i], 512) for i in range(3)]\n",
    "seqs = torch.cat(seqs, dim=0).cuda()\n",
    "lens.insert(0, 0)\n",
    "cu_seqlens = torch.cumsum(torch.tensor(lens), dim=0).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0,  5, 13, 26], device='cuda:0'), [0, 5, 8, 13])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cu_seqlens, lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 512])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seqs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1152, 4])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_params.key_value_memory_dict[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 26, 512])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "inf_params = InferenceParams(max_seqlen=512, max_batch_size=1)\n",
    "out = model.forward(\n",
    "    seqs.unsqueeze(0),\n",
    "    # seqlen=max_seq_len,\n",
    "    cu_seqlens=cu_seqlens,\n",
    "    inference_params=inf_params,\n",
    ")\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1152, 4])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_params.key_value_memory_dict[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 512])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unpack(out, cu_seqlens).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([26, 512])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out = model.forward(u=seqs, seqlen=13, cu_seqlens=cu_seqlens) \n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of params; 37025856\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "vocab_size = 32000\n",
    "d_model = 512\n",
    "n_layer = 12\n",
    "rms_norm = True\n",
    "fused_add_norm = True\n",
    "use_fast_path = False\n",
    "dropout = 0.1\n",
    "device = None\n",
    "\n",
    "\n",
    "cfg = MambaConfig(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    n_layer=n_layer,\n",
    "    rms_norm=rms_norm,\n",
    "    fused_add_norm=fused_add_norm,\n",
    "    use_fast_path=use_fast_path,\n",
    "    ssm_cfg={\"layer\": \"Mamba2\", \n",
    "             \n",
    "             \"dropout\": dropout\n",
    "             },\n",
    ")\n",
    "\n",
    "model = MambaLMHeadModel(\n",
    "    device=device,\n",
    "    config=cfg,\n",
    ").cuda()\n",
    "\n",
    "print(f\"nr of params; {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.backbone.embedding.weight.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 16, 32, 48, 68])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens = [0, 16, 16, 16, 20]\n",
    "torch.cumsum(torch.tensor(lens), dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(logits=tensor([[[-0.4535, -0.1903,  0.1940,  ..., -0.2765,  0.3405,  0.3049],\n",
       "         [-0.1870,  0.6263, -0.1692,  ...,  0.1917, -0.3314, -0.7155],\n",
       "         [ 0.2659,  0.2048, -0.0832,  ..., -0.2611, -0.6467, -0.5289],\n",
       "         ...,\n",
       "         [-0.7333,  0.0273,  0.1092,  ..., -0.2086,  0.0383,  0.5336],\n",
       "         [ 0.3112, -0.3334,  0.1502,  ..., -0.3022,  0.1446,  0.5136],\n",
       "         [ 0.2224,  0.5957,  0.2837,  ..., -1.2423,  0.3974,  0.1422]],\n",
       "\n",
       "        [[-0.4915,  0.5788, -0.0796,  ..., -0.1596,  0.0389,  0.2409],\n",
       "         [-0.7328,  0.0601,  0.0737,  ..., -0.2258,  0.0514,  0.4765],\n",
       "         [ 0.3146, -0.3397,  0.1267,  ..., -0.3161,  0.1515,  0.5114],\n",
       "         ...,\n",
       "         [-0.2585, -0.1724, -0.2640,  ..., -0.3327, -0.0336,  0.0461],\n",
       "         [-0.7257,  0.0241,  0.7068,  ..., -0.5366,  1.1969, -0.5112],\n",
       "         [ 0.0372,  0.0156, -0.3576,  ..., -0.4546, -0.2252,  0.7202]],\n",
       "\n",
       "        [[-0.5095,  0.2670,  0.2471,  ..., -0.9624,  0.4414,  0.2995],\n",
       "         [-0.2678, -0.1708, -0.2763,  ..., -0.3273, -0.0158,  0.0539],\n",
       "         [-0.7178,  0.0275,  0.7199,  ..., -0.5382,  1.2610, -0.5148],\n",
       "         ...,\n",
       "         [-0.2089,  0.7504, -0.0407,  ..., -0.2543, -0.2332, -0.2166],\n",
       "         [ 0.0837, -0.0409,  1.1160,  ...,  0.6092,  0.3862,  0.3066],\n",
       "         [ 0.3039,  0.4638,  0.5735,  ..., -0.4262,  0.2136, -0.2907]],\n",
       "\n",
       "        [[ 0.5920,  0.2171, -0.7595,  ...,  0.0709, -0.1521,  0.3803],\n",
       "         [-0.2204,  0.7556, -0.0261,  ..., -0.2944, -0.2346, -0.2012],\n",
       "         [ 0.1079, -0.0260,  1.1019,  ...,  0.6100,  0.3868,  0.2994],\n",
       "         ...,\n",
       "         [-0.6678,  1.2763, -0.5219,  ..., -0.1082,  0.8500,  0.9254],\n",
       "         [ 0.1388, -0.7748, -0.3384,  ..., -0.3691,  0.4039,  0.2415],\n",
       "         [-0.1727,  0.3403, -0.1991,  ...,  0.0898,  0.4073, -0.2774]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch = torch.randn(4, 32, 512).cuda()\n",
    "batch = torch.randint(0, 32000, (4, 20)).cuda()\n",
    "lens = [0, 16, 16, 16, 20]\n",
    "cu_seqlens = torch.cumsum(torch.tensor(lens), dim=0).cuda()\n",
    "max_seqlen = max(lens)\n",
    "\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "inf_params = InferenceParams(max_seqlen=512, max_batch_size=1)\n",
    "out = model.forward(\n",
    "    input_ids=batch,\n",
    "    # seqlen=max_seqlen,\n",
    "    cu_seqlens=cu_seqlens,\n",
    "    inference_params=inf_params,\n",
    ")\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20, 32000])"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "batch_size = len(seq_lengths)\n",
    "batch_indices = torch.arange(batch_size, device=cu_seqlens.device)\n",
    "last_token_logits = out.logits[batch_indices, seq_lengths - 1]\n",
    "# last_token_logits.shape\n",
    "next_tokens = torch.argmax(last_token_logits, dim=-1, keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 32000])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_token_logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 20])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 5948, 29903,  4018, 29128], device='cuda:0')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[batch_indices, seq_lengths-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1443, 27456, 20415,  3087,  6465,  8025, 25972,  2599,   551, 27948,\n",
       "         17773, 20349, 27143,   428,  9126,  5948,  3558,  8070, 18483, 17954,\n",
       "         24459],\n",
       "        [20300, 10746, 28850, 23706, 12792, 10966,  1608, 19539,  1391, 30376,\n",
       "          2586,  4297, 31050, 17172, 16162, 29903, 11419, 16579,  9919, 13000,\n",
       "          1777],\n",
       "        [16963, 27322, 29404, 31576,  3638, 14424,  4787, 10924,   956,  4131,\n",
       "          6685, 14701,  9244, 22546, 22311,  4018,  8396,  2757, 28358, 28719,\n",
       "         28254],\n",
       "        [23937, 29644,  3785, 29132,  9224, 24593, 17456,   906, 21355, 14527,\n",
       "          8706, 16524,  7036,    20, 11992,  7826, 23393, 17528,  9246, 29128,\n",
       "         18180]], device='cuda:0')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat((batch, next_tokens), dim=1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CausalLMOutput(logits=tensor([[[-0.1875, -1.3006,  0.3607,  ...,  0.4449, -0.9811,  1.2702]],\n",
       "\n",
       "        [[ 0.3344,  0.2296, -0.4904,  ...,  0.1901,  0.4278, -0.2156]],\n",
       "\n",
       "        [[ 0.7879, -0.0104,  0.2826,  ..., -0.3183, -0.3512,  0.2976]],\n",
       "\n",
       "        [[-1.1600,  0.0934, -0.2397,  ..., -0.6869, -0.2997,  0.2996]]],\n",
       "       device='cuda:0', grad_fn=<UnsafeViewBackward0>))"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_params.seqlen_offset +=1\n",
    "reout = model.forward(input_ids=next_tokens, inference_params=inf_params) \n",
    "reout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 32000])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reout.logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/transformers/utils/hub.py:127: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# autoreload\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from mamba_ssm.models.mixer_seq_simple import MambaLMHeadModel\n",
    "from mamba_ssm.models.config_mamba import MambaConfig\n",
    "from mamba_ssm.utils.generation import InferenceParams\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "from mt.ds import build_dataset\n",
    "\n",
    "ds = build_dataset(\n",
    "    name=\"iwslt17\",\n",
    "    source=\"de\",\n",
    "    target=\"en\",\n",
    "    is_encoder_decoder=False,\n",
    ")\n",
    "tokenizer = ds.get_tokenizer()\n",
    "tokenizer.padding_side = \"right\"\n",
    "tdl, vdl, _ = ds.get_dataloaders(\n",
    "    tokenizer=tokenizer, train_batch_size=2, val_batch_size=2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please install tensorboardX: pip install tensorboardX\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "models.mamba2.mt_wrapper.Mamba2MT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from models.factory import build_model\n",
    "import torch\n",
    "model_cls = build_model(\n",
    "    task=\"mt\",\n",
    "    name=\"mamba2\",\n",
    ")\n",
    "model_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nr of params:  78308544\n"
     ]
    }
   ],
   "source": [
    "mt = model_cls(dropout=0, tokenizer=tokenizer, vocab_size=32000, precision='bf16-mixed', **model_cls.configs[\"default\"]) \n",
    "mt = mt.to(\"cuda\").to(torch.bfloat16)\n",
    "print(\"nr of params: \", sum(p.numel() for p in mt.parameters() if p.requires_grad))\n",
    "device = \"cuda\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "for i,b in enumerate(tdl):\n",
    "    b = {k: v.to(device) for k,v in b.items()}\n",
    "    out = mt.training_step(b, i)  \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  569,   770,   440,  2427, 25064,    15,   675,   683,   621,  9037,\n",
       "          7452,    15,   304,   356,   427,  4119,   528,    17,     2,  1150,\n",
       "           372,   980,   342,   480,   295,  3570,    15,   618,   570,   372,\n",
       "           265,  1877,  1017,   336,   342,   447,   538,   480,   295,  3570,\n",
       "            17,     1],\n",
       "        [  500, 14460,   706, 19765,    17,     2,   296,  1523,   307, 19765,\n",
       "            17,     1,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 47])\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hugo/.pyenv/versions/3.10.4/envs/ctxeff/lib/python3.10/site-packages/pytorch_lightning/core/module.py:436: You are trying to `self.log()` but the `self.trainer` reference is not registered on the model yet. This is most likely because the model hasn't been passed to the `Trainer`\n"
     ]
    }
   ],
   "source": [
    "for i,b in enumerate(vdl):\n",
    "    b = {k: v.to(device) for k, v in b.items()}\n",
    "    print(b[\"input_ids\"].shape)\n",
    "    out = mt.validation_step(b, i)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[15548,   777,   830,   392,   594,  3407, 22629,  4699,    15,   548,\n",
       "           355, 13407, 10964,   873,    15,   434,   304,   612,   488,   863,\n",
       "           348,  2358, 20699,    15,   304,   531, 23442,  1510,  1908,  1261,\n",
       "           304, 24216,   349, 11637, 13846,  4035,  1033,    15,   548,  2369,\n",
       "          1904,  1283,  2789,  7134,   360,    17,     2],\n",
       "        [  696,   525, 19376,   427,  4031,  2725,   304, 14380, 22632,  1050,\n",
       "         12002, 13093,   541,    15,   587,   396,   427,   304,   337,  9032,\n",
       "           558,  4501,   272,  2746,    17,     2,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "             0,     0,     0,     0,     0,     0,     0]], device='cuda:0')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "You're using a PreTrainedTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n",
      "100\n",
      "101\n",
      "102\n",
      "103\n",
      "104\n",
      "105\n",
      "106\n",
      "107\n",
      "108\n",
      "109\n",
      "110\n",
      "111\n",
      "112\n",
      "113\n",
      "114\n",
      "115\n",
      "116\n",
      "117\n",
      "118\n",
      "119\n",
      "120\n",
      "121\n",
      "122\n",
      "123\n",
      "124\n",
      "125\n",
      "126\n",
      "127\n",
      "128\n",
      "129\n",
      "130\n",
      "131\n",
      "132\n",
      "133\n",
      "134\n",
      "135\n",
      "136\n",
      "137\n",
      "138\n",
      "139\n",
      "140\n",
      "141\n",
      "142\n",
      "143\n",
      "144\n",
      "145\n",
      "146\n",
      "147\n",
      "148\n",
      "149\n",
      "150\n",
      "151\n",
      "152\n",
      "153\n",
      "154\n",
      "155\n",
      "156\n",
      "157\n",
      "158\n",
      "159\n",
      "160\n",
      "161\n",
      "162\n",
      "163\n",
      "164\n",
      "165\n",
      "166\n",
      "167\n",
      "168\n",
      "169\n",
      "170\n",
      "171\n",
      "172\n",
      "173\n",
      "174\n",
      "175\n",
      "176\n",
      "177\n",
      "178\n",
      "179\n",
      "180\n",
      "181\n",
      "182\n",
      "183\n",
      "184\n",
      "185\n",
      "186\n",
      "187\n",
      "188\n",
      "189\n",
      "190\n",
      "191\n",
      "192\n",
      "193\n",
      "194\n",
      "195\n",
      "196\n",
      "197\n",
      "198\n",
      "199\n",
      "200\n",
      "201\n",
      "202\n",
      "203\n",
      "204\n",
      "205\n",
      "206\n",
      "207\n",
      "208\n",
      "209\n",
      "210\n",
      "211\n",
      "212\n",
      "213\n",
      "214\n",
      "215\n",
      "216\n",
      "217\n",
      "218\n",
      "219\n",
      "220\n",
      "221\n",
      "222\n",
      "223\n",
      "224\n",
      "225\n",
      "226\n",
      "227\n",
      "228\n",
      "229\n",
      "230\n",
      "231\n",
      "232\n",
      "233\n",
      "234\n",
      "235\n",
      "236\n",
      "237\n",
      "238\n",
      "239\n",
      "240\n",
      "241\n",
      "242\n",
      "243\n",
      "244\n",
      "245\n",
      "246\n",
      "247\n",
      "248\n",
      "249\n",
      "250\n",
      "251\n",
      "252\n",
      "253\n",
      "254\n",
      "255\n",
      "256\n",
      "257\n",
      "258\n",
      "259\n",
      "260\n",
      "261\n",
      "262\n",
      "263\n",
      "264\n",
      "265\n",
      "266\n",
      "267\n",
      "268\n",
      "269\n",
      "270\n",
      "271\n",
      "272\n",
      "273\n",
      "274\n",
      "275\n",
      "276\n",
      "277\n",
      "278\n",
      "279\n",
      "280\n",
      "281\n",
      "282\n",
      "283\n",
      "284\n",
      "285\n",
      "286\n",
      "287\n",
      "288\n",
      "289\n",
      "290\n",
      "291\n",
      "292\n",
      "293\n",
      "294\n",
      "295\n",
      "296\n",
      "297\n",
      "298\n",
      "299\n",
      "300\n",
      "301\n",
      "302\n",
      "303\n",
      "304\n",
      "305\n",
      "306\n",
      "307\n",
      "308\n",
      "309\n",
      "310\n",
      "311\n",
      "312\n",
      "313\n",
      "314\n",
      "315\n",
      "316\n",
      "317\n",
      "318\n",
      "319\n",
      "320\n",
      "321\n",
      "322\n",
      "323\n",
      "324\n",
      "325\n",
      "326\n",
      "327\n",
      "328\n",
      "329\n",
      "330\n",
      "331\n",
      "332\n",
      "333\n",
      "334\n",
      "335\n",
      "336\n",
      "337\n",
      "338\n",
      "339\n",
      "340\n",
      "341\n",
      "342\n",
      "343\n",
      "344\n",
      "345\n",
      "346\n",
      "347\n",
      "348\n",
      "349\n",
      "350\n",
      "351\n",
      "352\n",
      "353\n",
      "354\n",
      "355\n",
      "356\n",
      "357\n",
      "358\n",
      "359\n",
      "360\n",
      "361\n",
      "362\n",
      "363\n",
      "364\n",
      "365\n",
      "366\n",
      "367\n",
      "368\n",
      "369\n",
      "370\n",
      "371\n",
      "372\n",
      "373\n",
      "374\n",
      "375\n",
      "376\n",
      "377\n",
      "378\n",
      "379\n",
      "380\n",
      "381\n",
      "382\n",
      "383\n",
      "384\n",
      "385\n",
      "386\n",
      "387\n",
      "388\n",
      "389\n",
      "390\n",
      "391\n",
      "392\n",
      "393\n",
      "394\n",
      "395\n",
      "396\n",
      "397\n",
      "398\n",
      "399\n",
      "400\n",
      "401\n",
      "402\n",
      "403\n",
      "404\n",
      "405\n",
      "406\n",
      "407\n",
      "408\n",
      "409\n",
      "410\n",
      "411\n",
      "412\n",
      "413\n",
      "414\n",
      "415\n",
      "416\n",
      "417\n",
      "418\n",
      "419\n",
      "420\n",
      "421\n",
      "422\n",
      "423\n",
      "424\n",
      "425\n",
      "426\n",
      "427\n",
      "428\n",
      "429\n",
      "430\n",
      "431\n",
      "432\n",
      "433\n",
      "434\n",
      "435\n",
      "436\n",
      "437\n",
      "438\n",
      "439\n",
      "440\n",
      "441\n",
      "442\n",
      "443\n",
      "444\n",
      "445\n",
      "446\n",
      "447\n",
      "448\n",
      "449\n",
      "450\n",
      "451\n",
      "452\n",
      "453\n",
      "454\n",
      "455\n",
      "456\n",
      "457\n",
      "458\n",
      "459\n",
      "460\n",
      "461\n",
      "462\n",
      "463\n",
      "464\n",
      "465\n",
      "466\n",
      "467\n",
      "468\n",
      "469\n",
      "470\n",
      "471\n",
      "472\n",
      "473\n",
      "474\n",
      "475\n",
      "476\n",
      "477\n",
      "478\n",
      "479\n",
      "480\n",
      "481\n",
      "482\n",
      "483\n",
      "484\n",
      "485\n",
      "486\n",
      "487\n",
      "488\n",
      "489\n",
      "490\n",
      "491\n",
      "492\n",
      "493\n",
      "494\n",
      "495\n",
      "496\n",
      "497\n",
      "498\n",
      "499\n",
      "500\n",
      "501\n",
      "502\n",
      "503\n",
      "504\n",
      "505\n",
      "506\n",
      "507\n",
      "508\n",
      "509\n",
      "510\n",
      "511\n"
     ]
    }
   ],
   "source": [
    "for i,b in enumerate(vdl):\n",
    "\n",
    "    b = {k: v.to(device) for k, v in b.items()}\n",
    "\n",
    "    input_ids, labels = b[\"input_ids\"], b[\"labels\"]\n",
    "    batch_size, seq_len = input_ids.shape\n",
    "    max_length = 512\n",
    "    attention_mask = b[\"attention_mask\"] if mt.use_padding else None\n",
    "\n",
    "    cu_seqlens = torch.zeros(\n",
    "        batch_size + 1, dtype=torch.int32, device=attention_mask.device\n",
    "    )\n",
    "    cu_seqlens[1:] = attention_mask.sum(dim=1, dtype=torch.int32).cumsum(0)\n",
    "\n",
    "    cache = mt.model.allocate_inference_cache(\n",
    "        batch_size=batch_size,\n",
    "        max_seqlen=max_length + seq_len,\n",
    "        dtype=mt.precision,\n",
    "    )\n",
    "    inference_params = InferenceParams(\n",
    "        max_seqlen=max_length + seq_len,\n",
    "        max_batch_size=batch_size,\n",
    "        key_value_memory_dict=cache,\n",
    "    )\n",
    "    done = torch.tensor([False] * batch_size).to(input_ids.device)\n",
    "\n",
    "    for idx in range(max_length):\n",
    "        print(idx)\n",
    "        if idx > 0:\n",
    "            last_tokens = input_ids[:, -1:]  # (B, 1)\n",
    "\n",
    "        outputs = mt.model.forward(\n",
    "            input_ids=input_ids if idx == 0 else last_tokens,\n",
    "            cu_seqlens=cu_seqlens if idx == 0 else None,\n",
    "            inference_params=inference_params,\n",
    "        ).logits\n",
    "\n",
    "\n",
    "        if idx == 0:\n",
    "            seq_lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "            batch_indices = torch.arange(outputs.shape[0], device=outputs.device)\n",
    "            next_token_logits = outputs[batch_indices, seq_lengths - 1]\n",
    "        else:\n",
    "            next_token_logits = outputs.squeeze(1)\n",
    "\n",
    "        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)\n",
    "        input_ids = torch.cat((input_ids, next_token), dim=-1)\n",
    "        inference_params.seqlen_offset += 1\n",
    "\n",
    "        is_eos = next_token == mt.tokenizer.eos_token_id\n",
    "        done = done | is_eos.squeeze(-1)\n",
    "        if done.all():\n",
    "            break\n",
    "\n",
    "    # Create a cumulative sum mask where positions after EOS become True\n",
    "    eos_token_id = mt.tokenizer.eos_token_id\n",
    "    eos_mask = (input_ids == eos_token_id).cumsum(dim=1) > 0\n",
    "    input_ids[eos_mask] = mt.tokenizer.pad_token_id\n",
    "\n",
    "    # mask source sentence\n",
    "    source_mask = (input_ids == mt.tokenizer.sep_token_id).cumsum(dim=1) == 0\n",
    "    input_ids[source_mask] = mt.tokenizer.pad_token_id\n",
    "\n",
    "    tpreds = mt.tokenizer.batch_decode(input_ids, skip_special_tokens=True)\n",
    "    tlabels = mt.tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    bleu_score = mt.bleu.compute(predictions=tpreds, references=tlabels)[\"score\"]\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.3711,  0.2197, -0.4512,  ..., -0.6328, -0.5195,  0.1167],\n",
       "        [ 0.1260, -0.2188, -0.3457,  ...,  0.1221, -0.2266, -0.2451]],\n",
       "       device='cuda:0', dtype=torch.bfloat16, grad_fn=<SqueezeBackward1>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next_token_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 47, 32000]),\n",
       " tensor([47, 26], device='cuda:0', dtype=torch.int32))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seq_lengths = cu_seqlens[1:] - cu_seqlens[:-1]\n",
    "outputs.shape, seq_lengths\n",
    "\n",
    "\"\"\"(torch.Size([2, 47, 32000]),\n",
    " tensor([47, 26], device='cuda:0', dtype=torch.int32))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'torch.Size([2, 2, 32000])\\n'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[:, seq_lengths - 1].shape\n",
    "\n",
    "\"\"\"torch.Size([2, 2, 32000])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_indices = torch.arange(outputs.shape[0], device=outputs.device)\n",
    "last_token_indices = seq_lengths - 1\n",
    "result = outputs[batch_indices, last_token_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 32000])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.argmax(dim=-1, keepdim=True).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
